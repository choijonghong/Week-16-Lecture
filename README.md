# Week-16-Lecture
Week-16-Lecture

### 왜 Transformer가 인공지능 발전의 ‘게임 체인저’로 불리는가?
* 논문명:Attention Is All You Need (2017)
* 순서 따지지 않고 모든 단어의 관계를 한 번에 계산하는 AI 모델
  

### 논문배경
* 기존 자연어 번역 AI는 대부분 RNN(LSTM, GRU) 기반이었고,
* 입력을 순서대로 처리해야 해서 훈련이 느리고 긴 문장은 멀리 떨어진 단어 관계를 파악하기 어려웠습니다.
* CNN 기반 모델도 등장했지만 멀리 떨어진 단어 관계를 연결하려면 많은 층이 필요했습니다.
* 속도 문제와 장거리 의존성(문장 내 멀리 있는 단어 관계) 학습이 큰 문제였으며, 이를 해결할 새로운 알고리즘이 필요했습니다.
* 이러한 문제를 해결하기 위해 순서 따지지 않고 모든 단어의 관계를 한 번에 계산하는 Transformer(Attention) 모델이 등장했습니다.

### 기존 연구의 한계점
* 속도가 느립니다 (RNN): 단어를 순서대로 하나씩 처리해야 해서, 동시에 여러 계산을 하는 병렬 처리가 불가능했습니다.
* 기억력이 짧습니다 (RNN): 문장이 길어지면 앞에 나온 단어와 뒤에 나온 단어의 관계를 잘 파악하지 못했습니다.
* 구조가 복잡합니다 (CNN): 멀리 떨어진 단어끼리 연결하려면 모델을 아주 깊게 쌓아야만 했습니다.

### 논문 목표
* 복잡한 연산이 필요한 RNN과 CNN을 완전히 배제하고, 오직 Attention(주의 집중) 메커니즘만으로 작동하는 새로운 모델 구조를 제안한다.
* 효율성 및 성능 극대화: 데이터의 완전 병렬 처리를 구현하여 학습 속도를 획기적으로 높이고, 문장의 길이에 상관없이 단어 간의 관계를 정확히 파악하여 번역 품질을 향상시킨다.

### 논문내용(주요 혁신 및 방법론)  

"순서대로 기다리지 않고 문장 전체를 한눈에 파악하며(병렬 처리), 여러 개의 눈으로 단어 사이의 미세한 관계까지 놓치지 않는(Multi-Head Attention) 혁신적인 모델입니다."

<img width="300" height="500" alt="image" src="https://github.com/user-attachments/assets/c76fc4b4-bb7c-4f88-80e3-7a4b8a79dd03" />


* 위치 정보 주입: Positional Encoding
  * Transformer는 문장을 처음부터 끝까지 순서대로 읽지 않습니다.
  * 문장 전체를 한 번에 입력받기 때문에 단어의 순서를 모릅니다.
  * 그래서 각 단어에 '순서 번호'를 매겨주는 수학적 신호를 추가합니다.

* 핵심 원리: Self-Attention (자기 주의 집중)
  * 단어의 의미는 문맥에 따라 결정됩니다. (예: '배'는 먹는 배? 타는 배?)
  * 이 모델은 문장 내의 모든 단어가 서로를 쳐다보게 합니다.
  * 각 단어가 서로 얼마나 밀접한 관계인지 계산하여 문맥을 파악합니다.

* 계산 방식: Scaled Dot-Product Attention
  *   <img width="199" height="199" alt="image" src="https://github.com/user-attachments/assets/4df60418-b30e-49e1-a0ff-3196e9bd76c6" />

  * 단어 간의 관계를 수학적으로 계산(내적)하는 구체적인 방법입니다.
  * 값이 너무 커지면 학습이 불안정해질 수 있습니다.
  * 따라서 계산 값을 적절한 크기로 나누어(Scaling) 빠르고 안정적인 학습을 돕습니다.

* 다각도 분석: Multi-Head Attention
  *   <img width="213" height="221" alt="image" src="https://github.com/user-attachments/assets/b4136df2-fabb-4f3e-85fc-2c99ff251b44" />

  * 문장을 한 가지 관점으로만 보면 놓치는 정보가 생깁니다.
  * 여러 개의 Attention(헤드)을 두어 동시에 여러 관점으로 문장을 분석합니다.
  * 어떤 헤드는 문법을, 어떤 헤드는 의미를 파악하여 이해력을 높입니다.

* 효율성: 완전 병렬 구조
  * 앞 단어의 계산이 끝날 때까지 기다릴 필요가 없습니다.
  * 모든 단어의 관계를 동시에 계산합니다.
  * GPU의 능력을 100% 활용하여 학습 속도가 기존보다 압도적으로 빠릅니다.

### 기여도 및 결과
* Transformer는 영→독, 영→불 번역에서 기존 최고 성능 모델을 모두 넘어서는 성과를 기록했습니다.
* 기존 RNN·CNN 기반 모델은 순차 처리 때문에 시간이 오래 걸렸습니다, 하지만 Transformer는 병렬 처리로 학습 시간을 대폭 단축했습니다.
* Transformer는 RNN과 CNN 없이 Attention만으로 언어를 이해하고 생성할 수 있음을 증명했습니다.
* 이 구조는 이후 BERT, GPT, DALL·E 등 모든 생성형 AI의 기본 설계가 되었습니다.

### 감성컴퓨팅과 Transformer
* 감성컴퓨팅은 사람의 감정·뉘앙스·태도 같은 비언어적 의미를 이해하는 AI를 목표로 한다.
* 기존 RNN 기반 모델은 문맥의 미세한 변화나 긴 대화 맥락에서의 감정 흐름 파악이 제한적이었다.
* Transformer의 Self-Attention은 문장 전체 단어를 동시에 비교하여 감정의 전환, 강조, 반전 등을 정밀하게 탐지할 수 있게 한다.
* Multi-Head Attention은 감정·주제·대상·의도 등 복수의 의미 층을 동시에 해석하는 데 유리하다.
* 그 결과, Transformer는 감성 분석, 감정 기반 추천, 상담 챗봇, 생성형 대화 등 감성컴퓨팅 핵심 분야의 정확도와 자연스러움을 크게 향상시켰다.
* 요약하면, Transformer는 문장 전체를 바라보며 의미를 분석하는 구조 덕분에, 감정과 뉘앙스처럼 미세한 맥락을 이해해야 하는 감성컴퓨팅에 최적화된 기반 기술이다.

### 참고 파일

* 논문1. Attention is All You Need


