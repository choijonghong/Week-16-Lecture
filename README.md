# Week-16-Lecture
Week-16-Lecture

### 왜 Transformer가 인공지능 발전의 ‘게임 체인저’로 불리는가?
* 논문명:Attention Is All You Need (2017)
* 순서 따지지 않고 모든 단어의 관계를 한 번에 계산하는 AI 모델
  

### 논문배경
* GAN의 기술적 성숙: ProGAN 등의 등장으로 고해상도(1024x1024) 이미지 생성 가능성 입증
* 생성 과정의 불투명성: 이미지 합성 과정에서 각 특징(Feature)이 어떻게 형성되는지 알 수 없는 '블랙박스' 아키텍처의 한계
* 제어권 부족: 생성된 이미지의 특정 요소(예: 머리색, 포즈, 피부톤)를 독립적으로 수정할 수 있는 방법론 부재
* 잠재 공간 이해도 저하: 잠재 공간(Latent Space)의 기하학적 특성과 이미지 특징 간의 상관관계에 대한 이론적·실험적 분석 부족

### 기존 연구의 한계점
* 전통적 생성기 구조의 제약:
  * 입력 잠재 코드($z$)가 네트워크의 시작 부분(Input layer)에만 투입되어 하위 레이어로 갈수록 그 영향력이 희석되거나 비선형적으로 변질됨
* 잠재 공간의 얽힘(Entanglement) 문제:
  * 입력 공간 $Z$는 훈련 데이터의 확률 밀도 분포를 강제로 따라야 함
  * 이로 인해 데이터에 존재하지 않는 조합을 피하기 위해 잠재 공간이 왜곡(Warping)되며, 결과적으로 여러 특징이 복합적으로 얽히게 됨
* 확률적 요소 처리의 비효율성:
  * 머리카락의 미세한 위치, 모공, 주근깨와 같은 '확률적(Stochastic) 변동'을 생성하기 위해 귀중한 네트워크 용량을 낭비함
* 객관적 평가 지표 결여:
  * 생성된 이미지의 품질(FID 등) 외에, 잠재 공간의 선형성이나 특징 분리 정도를 측정할 수 있는 표준화된 지표가 없음

### 논문 목표
* 스타일 기반 아키텍처 설계: 스타일 트랜스퍼(Style Transfer) 개념을 도입하여 이미지 합성 과정을 명시적으로 제어
* 특징의 분리 및 제어: 고수준 속성(Pose, Identity)과 미세한 확률적 변동(Stochastic variation)의 완전한 분리 학습
* 잠재 공간의 선형성 확보: 훈련 데이터 분포의 제약에서 벗어난 중간 잠재 공간($W$) 구축을 통한 디스엔탱글먼트(Disentanglement) 실현
* 고품질 데이터셋 제공: 기존 CelebA-HQ보다 높은 다양성과 품질을 가진 FFHQ 데이터셋 구축 및 공개

### 논문내용(주요 혁신 및 방법론)  
① 비선형 매핑 네트워크 (Mapping Network)
  * 구조: 8개 레이어의 MLP(Multi-Layer Perceptron)를 통해 $z \in Z$를 $w \in W$로 변환

효과: 입력 데이터의 분포 제약에서 벗어나 특징별로 선형적으로 분리된 중간 잠재 공간 $W$를 형성 (Unwarping 효과)


### 기여도 및 결과
* Transformer는 영→독, 영→불 번역에서 기존 최고 성능 모델을 모두 넘어서는 성과를 기록했습니다.
* 기존 RNN·CNN 기반 모델은 순차 처리 때문에 시간이 오래 걸렸습니다, 하지만 Transformer는 병렬 처리로 학습 시간을 대폭 단축했습니다.
* Transformer는 RNN과 CNN 없이 Attention만으로 언어를 이해하고 생성할 수 있음을 증명했습니다.
* 이 구조는 이후 BERT, GPT, DALL·E 등 모든 생성형 AI의 기본 설계가 되었습니다.

### 감성컴퓨팅과 Transformer
* 감성컴퓨팅은 사람의 감정·뉘앙스·태도 같은 비언어적 의미를 이해하는 AI를 목표로 한다.
* 기존 RNN 기반 모델은 문맥의 미세한 변화나 긴 대화 맥락에서의 감정 흐름 파악이 제한적이었다.
* Transformer의 Self-Attention은 문장 전체 단어를 동시에 비교하여 감정의 전환, 강조, 반전 등을 정밀하게 탐지할 수 있게 한다.
* Multi-Head Attention은 감정·주제·대상·의도 등 복수의 의미 층을 동시에 해석하는 데 유리하다.
* 그 결과, Transformer는 감성 분석, 감정 기반 추천, 상담 챗봇, 생성형 대화 등 감성컴퓨팅 핵심 분야의 정확도와 자연스러움을 크게 향상시켰다.
* 요약하면, Transformer는 문장 전체를 바라보며 의미를 분석하는 구조 덕분에, 감정과 뉘앙스처럼 미세한 맥락을 이해해야 하는 감성컴퓨팅에 최적화된 기반 기술이다.

### 참고 파일

* 논문1. Attention is All You Need


